{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnhMoI4xI2m0",
        "outputId": "69b10ddd-fd22-4967-d8ad-808777cd3968"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da1807c-cbbb-472f-8ab3-8442ca5ab9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0                                                  1           2  \\\n",
            "0                                             Name_Info  Noun_Count   \n",
            "1  0  kay aiko abe nisei femal born may selleck wash...          10   \n",
            "2  1  art abe nisei male born june seattl washington...          10   \n",
            "3  2  sharon tanagi aburano nisei femal born octob s...          14   \n",
            "4  3  toshiko aiboshi nisei femal born juli boyl hei...          11   \n",
            "\n",
            "            3          4          5  \n",
            "0  Verb_Count  Adj_Count  Adv_Count  \n",
            "1           3          2          1  \n",
            "2           2          2          0  \n",
            "3           1          4          0  \n",
            "4           2          3          1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "\n",
        "with open('/root/Info_5731 (1).csv', 'r') as f:\n",
        "    data = list(csv.reader(f))\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhwqdWSYNVIK",
        "outputId": "12bdc95f-f4dc-42ea-cbe6-1053b52805d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Using cached pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.5.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "NyvbuQAqNY7L",
        "outputId": "6b84ad35-65ef-4e4a-fa70-0d64c446cbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Using cached pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.1\n",
            "    Uninstalling pandas-2.2.1:\n",
            "      Successfully uninstalled pandas-2.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "381967b09317426590e7164b0e0b3b05"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "import re\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read data into news_articles DataFrame\n",
        "news_articles = pd.read_csv('/root/Info_5731 (1).csv')\n",
        "\n",
        "# Preprocess text data\n",
        "news_articles['processed_text'] = news_articles['Name_Info'].map(lambda x: re.sub('[,!?]', '', x))\n",
        "news_articles['processed_text'] = news_articles['processed_text'].map(lambda x: x.lower())\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return [word for word in simple_preprocess(text) if word not in stop_words]\n",
        "\n",
        "news_articles['tokenized_text'] = news_articles['processed_text'].map(tokenize)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "id2word = Dictionary(news_articles['tokenized_text'])\n",
        "corpus = [id2word.doc2bow(text) for text in news_articles['tokenized_text']]\n",
        "\n",
        "# Determine the optimal number of topics using coherence score\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, workers=1)  # Set workers to 1\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Set the range of topics to evaluate\n",
        "start_topic = 2\n",
        "end_topic = 11\n",
        "step_topic = 1\n",
        "\n",
        "# Compute coherence scores\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word,\n",
        "                                                        corpus=corpus,\n",
        "                                                        texts=news_articles['tokenized_text'],\n",
        "                                                        start=start_topic,\n",
        "                                                        limit=end_topic,\n",
        "                                                        step=step_topic)\n",
        "\n",
        "# Find the optimal number of topics based on coherence score\n",
        "optimal_num_topics = start_topic + coherence_values.index(max(coherence_values))\n",
        "\n",
        "# Build LDA model with optimal number of topics\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=optimal_num_topics, workers=1)  # Set workers to 1\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics()\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RRa-FExNhvm",
        "outputId": "347c9fdd-663f-44c0-a2a4-b0ae13a8dfd1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.038*\"washington\" + 0.037*\"born\" + 0.034*\"nisei\" + 0.024*\"seattl\" + 0.023*\"male\" + 0.015*\"father\" + 0.015*\"grew\" + 0.014*\"california\" + 0.012*\"famili\" + 0.011*\"femal\"')\n",
            "(1, '0.045*\"born\" + 0.033*\"nisei\" + 0.029*\"grew\" + 0.027*\"parent\" + 0.025*\"femal\" + 0.024*\"ran\" + 0.023*\"world\" + 0.022*\"war\" + 0.021*\"california\" + 0.020*\"ii\"')\n",
            "(2, '0.045*\"born\" + 0.029*\"nisei\" + 0.024*\"california\" + 0.024*\"femal\" + 0.020*\"male\" + 0.020*\"washington\" + 0.018*\"grew\" + 0.017*\"los\" + 0.017*\"angel\" + 0.015*\"war\"')\n",
            "(3, '0.053*\"born\" + 0.049*\"california\" + 0.041*\"nisei\" + 0.030*\"male\" + 0.027*\"grew\" + 0.024*\"world\" + 0.024*\"war\" + 0.022*\"femal\" + 0.020*\"ii\" + 0.016*\"remov\"')\n",
            "(4, '0.028*\"washington\" + 0.026*\"born\" + 0.026*\"seattl\" + 0.023*\"nisei\" + 0.021*\"male\" + 0.010*\"grew\" + 0.008*\"bill\" + 0.008*\"femal\" + 0.007*\"california\" + 0.007*\"bainbridg\"')\n",
            "(5, '0.031*\"born\" + 0.025*\"male\" + 0.023*\"japan\" + 0.013*\"school\" + 0.012*\"nisei\" + 0.011*\"father\" + 0.010*\"california\" + 0.008*\"return\" + 0.008*\"attend\" + 0.007*\"live\"')\n",
            "(6, '0.044*\"born\" + 0.035*\"nisei\" + 0.033*\"california\" + 0.030*\"grew\" + 0.025*\"male\" + 0.021*\"war\" + 0.021*\"femal\" + 0.020*\"world\" + 0.020*\"ii\" + 0.016*\"remov\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe60c27-7d34-4ce8-a0c8-22dbcf8756d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0                                                  1           2  \\\n",
            "0                                                 Name_Info  Noun_Count   \n",
            "1      0  kay aiko abe nisei femal born may selleck wash...          10   \n",
            "2      1  art abe nisei male born june seattl washington...          10   \n",
            "3      2  sharon tanagi aburano nisei femal born octob s...          14   \n",
            "4      3  toshiko aiboshi nisei femal born juli boyl hei...          11   \n",
            "..   ...                                                ...         ...   \n",
            "973  972  karen yoshitomi sansei femal born spokan washi...           6   \n",
            "974  973  john young chine american male born may los an...           8   \n",
            "975  974  sharon yuen sansei femal born juli seattl wash...           9   \n",
            "976  975  loi yuki nisei femal born septemb tule lake co...          11   \n",
            "977  976  aaron zajic born baltimor maryland redress mov...          11   \n",
            "\n",
            "              3          4          5  \n",
            "0    Verb_Count  Adj_Count  Adv_Count  \n",
            "1             3          2          1  \n",
            "2             2          2          0  \n",
            "3             1          4          0  \n",
            "4             2          3          1  \n",
            "..          ...        ...        ...  \n",
            "973           6          4          0  \n",
            "974           3          4          0  \n",
            "975           1          1          0  \n",
            "976           2          4          0  \n",
            "977           2          1          1  \n",
            "\n",
            "[978 rows x 6 columns]\n",
            "Coherence score with 2 clusters: 0.3729738388941557\n",
            "Coherence score with 3 clusters: 0.3515002670224972\n",
            "Coherence score with 4 clusters: 0.2429406601962693\n",
            "Coherence score with 5 clusters: 0.32318335257822434\n",
            "Coherence score with 6 clusters: 0.28552853117956734\n",
            "Coherence score with 7 clusters: 0.3150703096811383\n",
            "Coherence score with 8 clusters: 0.319432685541064\n",
            "Coherence score with 9 clusters: 0.3151362424616246\n",
            "Coherence score with 10 clusters: 0.29885052218496233\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short, stem_text, preprocess_string\n",
        "from gensim import corpora\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Assuming df is your DataFrame containing Name_Info column\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess(text):\n",
        "    CUSTOM_FILTERS = [lambda x: x.lower(),\n",
        "                      remove_stopwords,\n",
        "                      strip_punctuation,\n",
        "                      strip_short,\n",
        "                      stem_text]\n",
        "    text = preprocess_string(text, CUSTOM_FILTERS)\n",
        "    return text\n",
        "\n",
        "print(df)\n",
        "# Preprocess text data\n",
        "df['Text (Clean)'] = df.iloc[:, 1].apply(lambda x: preprocess(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a dictionary with the corpus\n",
        "corpus = df['Text (Clean)']\n",
        "dictionary = corpora.Dictionary(corpus)\n",
        "\n",
        "# Convert corpus into a bag of words\n",
        "bow = [dictionary.doc2bow(text) for text in corpus]\n",
        "\n",
        "# Find the optimal number of topics using coherence score\n",
        "coherence_values = []\n",
        "for num_topics in range(2, 11):\n",
        "    lsi = LsiModel(bow, num_topics=num_topics, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=df['Text (Clean)'], dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_values.append((num_topics, coherence_score))\n",
        "    print('Coherence score with {} clusters: {}'.format(num_topics, coherence_score))\n",
        "\n",
        "# Choose the optimal number of topics based on coherence score\n",
        "optimal_num_topics = max(coherence_values, key=lambda x: x[1])[0]\n",
        "\n",
        "# Perform LSA to generate K topics\n",
        "lsi = LsiModel(bow, num_topics=optimal_num_topics, id2word=dictionary)\n",
        "\n",
        "# Print the coherence score\n",
        "# print('Optimal number of topics:', optimal_num_topics)\n",
        "# print('Coherence score with {} clusters: {}'.format(optimal_num_topics, max(coherence_values, key=lambda x: x[1])[1]))\n",
        "\n",
        "# Print the 5 words with the strongest association to the derived topics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in topic {}: {}.'.format(topic_num, words))\n",
        "\n",
        "# Find the scores given between the review and each topic\n",
        "corpus_lsi = lsi[bow]\n",
        "scores = []\n",
        "for doc in corpus_lsi:\n",
        "    scores.append([round(val[1], 2) for val in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZZzEUyQfFc",
        "outputId": "67346cc9-ece0-4b5b-ce8a-1f1c2afb7613"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in topic 0: 0.457*\"born\" + 0.397*\"california\" + 0.377*\"nisei\" + 0.267*\"grew\" + 0.256*\"male\" + 0.222*\"world\" + 0.222*\"war\" + 0.205*\"femal\" + 0.156*\"remov\" + 0.152*\"washington\".\n",
            "Words in topic 1: 0.570*\"washington\" + -0.538*\"california\" + 0.392*\"seattl\" + -0.182*\"lo\" + -0.177*\"angel\" + 0.151*\"born\" + 0.123*\"nisei\" + 0.117*\"japan\" + -0.113*\"war\" + -0.111*\"world\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to show scores assigned for each topic for each review\n",
        "df_topic = pd.DataFrame(scores, columns=['Topic {}'.format(i) for i in range(optimal_num_topics)])\n",
        "print(df_topic)  # Corrected line\n",
        "df_topic['Text'] = df['Text (Clean)']  # Assuming 'Name_Info' is the correct column name\n",
        "df_topic['Dominant Topic'] = df_topic.iloc[:, :optimal_num_topics].idxmax(axis=1)\n",
        "\n",
        "\n",
        "# Find a sample review from each topic\n",
        "for i in range(optimal_num_topics):\n",
        "    df_topic_i = df_topic[df_topic['Dominant Topic'] == 'Topic {}'.format(i)]\n",
        "    if not df_topic_i.empty:\n",
        "        sample_text = df_topic_i.sample(1, random_state=2)['Text'].values[0]\n",
        "        print('Sample text from topic {}:\\n{}'.format(i, sample_text))\n",
        "    else:\n",
        "        print('No sample text available for topic {}'.format(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyY2McaLQh4Y",
        "outputId": "b164201e-1b51-48b9-eb0e-7ef865f21017"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Topic 0  Topic 1\n",
            "0       0.00     0.00\n",
            "1       1.49     1.12\n",
            "2       1.87     1.88\n",
            "3       2.00     1.16\n",
            "4       1.76    -0.10\n",
            "..       ...      ...\n",
            "973     2.34     1.70\n",
            "974     1.94    -0.99\n",
            "975     1.00     1.17\n",
            "976     2.21     0.08\n",
            "977     0.52     0.20\n",
            "\n",
            "[978 rows x 2 columns]\n",
            "Sample text from topic 0:\n",
            "['makoto', 'otsu', 'nisei', 'male', 'born', 'march', 'steveston', 'british', 'columbia', 'canada', 'grew', 'steveston', 'father', 'fish', 'canneri', 'world', 'war']\n",
            "Sample text from topic 1:\n",
            "['masako', 'murakami', 'nisei', 'femal', 'born', 'octob', 'seattl', 'washington', 'incarc', 'puyallup', 'assembl', 'center', 'washington', 'minidoka', 'concentr', 'camp', 'idaho', 'resettl', 'seattl', 'washington']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim pandas nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK7PplbrZQ0Q",
        "outputId": "12b7a606-29bb-4576-d9d7-b76d0586ef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Lda2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftGgSKVSZFdi",
        "outputId": "63fa76b2-54b2-4566-8f9e-c967da84c0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Lda2Vec in /usr/local/lib/python3.10/dist-packages (0.16.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "daab302550e64fea84067faa756b995c",
            "ce5dc7f7fcb249c583f9e260fded40e4",
            "78404ea4a1f54ea78bfa9b65022dbd95",
            "5ff90f66f7b6423f9894438fd9b92505",
            "6d8f3e44fab3455885374eea56564925",
            "96aef63cfc4a43f2bd88fa6565f3d614",
            "3df2f7745467456d987beae97f2c24aa",
            "743509651033474ebbe872b49597e95e",
            "9e0d1db9953640d09b05ecf37240b944",
            "5833f63d29c1425699d932b16fcc549a",
            "255ff9c961804183b10289739b56857f"
          ]
        },
        "outputId": "cab31e03-a5ec-4dc9-b93d-dd7f3d5e7138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-29 23:44:41,360 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daab302550e64fea84067faa756b995c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-29 23:44:58,818 - BERTopic - Embedding - Completed ✓\n",
            "2024-03-29 23:44:58,821 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-03-29 23:45:03,452 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-03-29 23:45:03,454 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-03-29 23:45:03,523 - BERTopic - Cluster - Completed ✓\n",
            "2024-03-29 23:45:03,530 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-29 23:45:03,576 - BERTopic - Representation - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.7319151928449092\n",
            "Number of Topics (K): 3\n",
            "\n",
            "Topic Summaries:\n",
            "Topic 0: born, nisei, california, male, grew, femal, world, war, ii, washington\n",
            "\n",
            "Topic 1: interview, tashima, bill, led, jacl, panel, elain, akemi, kuros, joy\n",
            "\n",
            "Topic 2: redress, justic, os, identifi, movement, offic, administr, depart, establish, work\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(\"/root/Info_5731 (1).csv\")\n",
        "\n",
        "# Initialize BERTopic model\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Convert values in 'Name_Info' column to strings\n",
        "df['Name_Info'] = df['Name_Info'].astype(str)\n",
        "\n",
        "# Fit BERTopic model on the preprocessed text data\n",
        "topics, probabilities = topic_model.fit_transform(df['Name_Info'])\n",
        "\n",
        "# Get top words per topic\n",
        "topics_info = topic_model.get_topics()\n",
        "top_words_per_topic = [[word for word, _ in words] for topic_id, words in topics_info.items() if topic_id != -1]\n",
        "\n",
        "# Tokenize and preprocess documents\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "processed_docs = [[word for word in tokenizer.tokenize(doc.lower()) if word not in stop_words] for doc in df['Name_Info'].astype(str).tolist()]\n",
        "\n",
        "# Create dictionary and corpus for coherence calculation\n",
        "dictionary = Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Calculate coherence score\n",
        "coherence_model = CoherenceModel(topics=top_words_per_topic, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n",
        "\n",
        "# Determine the number of topics\n",
        "num_topics = len(topics_info) - 1 if -1 in topics_info else len(topics_info)\n",
        "print(f\"Number of Topics (K): {num_topics}\")\n",
        "\n",
        "# Summarize Topics\n",
        "print(\"\\nTopic Summaries:\")\n",
        "for topic_id, words in topics_info.items():\n",
        "    if topic_id != -1:\n",
        "        topic_summary = \", \".join([word for word, _ in words])\n",
        "        print(f\"Topic {topic_id}: {topic_summary}\\n\")\n",
        "    else:\n",
        "        break  # No more topics to print\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install BERTopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT1kSQS2SJHe",
        "outputId": "04d8e0f8-1b12-462d-ef84-f4cb002a20d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting BERTopic\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m112.6/154.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from BERTopic) (1.25.2)\n",
            "Collecting hdbscan>=0.8.29 (from BERTopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from BERTopic)\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from BERTopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from BERTopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from BERTopic) (4.66.2)\n",
            "Collecting sentence-transformers>=0.4.1 (from BERTopic)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from BERTopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->BERTopic)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->BERTopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->BERTopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->BERTopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->BERTopic) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->BERTopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->BERTopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->BERTopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->BERTopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->BERTopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->BERTopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->BERTopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->BERTopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->BERTopic)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->BERTopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->BERTopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m807.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->BERTopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->BERTopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->BERTopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->BERTopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->BERTopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, umap-learn\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039283 sha256=673fd0adb2facc5e03ce007412f9e455c77e1dc1d68850991df6f6262c1adfed\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=b28a6b22438c7c1e97827a5e619c22022c80f65ccc9ce8b67c66ef56cec863a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built hdbscan umap-learn\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cython, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, hdbscan, umap-learn, sentence-transformers, BERTopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.9\n",
            "    Uninstalling Cython-3.0.9:\n",
            "      Successfully uninstalled Cython-3.0.9\n",
            "Successfully installed BERTopic-0.16.0 cython-0.29.37 hdbscan-0.8.33 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.12 sentence-transformers-2.6.1 umap-learn-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "d773b678b3d8462c849ba2bb6763e694",
            "b5f7688f39474ed9a0c72584ba8d59f9",
            "db698d5f26ef445c8cea1a1c259bba41",
            "bc9719208b0741c38a8e88d8dcabd6a0",
            "aa1457f4c5434d9389272a75b6fdee39",
            "7cdada3799434a4f83b1793a778c0c3e",
            "b483e866c51b4a7f9b1f051c6e216ecf",
            "24d7b72bb79f4e8bb9aa4f834b501975",
            "3e25e0658dc740a2ab301ff08e997a14",
            "8190cb0a00d44135a406c9c272a6789f",
            "40dd08595ed940a88d197bc627b42bfa"
          ]
        },
        "outputId": "2a60634b-62ee-48f8-fae4-a3bc29a0758b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-29 23:41:54,845 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d773b678b3d8462c849ba2bb6763e694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-29 23:42:17,255 - BERTopic - Embedding - Completed ✓\n",
            "2024-03-29 23:42:17,258 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-03-29 23:42:21,870 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-03-29 23:42:21,872 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-03-29 23:42:21,944 - BERTopic - Cluster - Completed ✓\n",
            "2024-03-29 23:42:21,952 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-29 23:42:22,006 - BERTopic - Representation - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.7209625284078601\n",
            "Number of Topics (K): 3\n",
            "\n",
            "Topic Summaries:\n",
            "Topic 0: born, nisei, california, male, grew, femal, world, war, washington, remov\n",
            "\n",
            "Topic 1: interview, tashima, led, jacl, panel, elain, akemi, joi, kuro, matsumoto\n",
            "\n",
            "Topic 2: redress, justic, identifi, offic, administr, movement, depart, establish, work, administ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Initialize BERTopic model\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Assuming df is your DataFrame containing the data\n",
        "# Ensure 'Text (Clean)' column is present in your DataFrame\n",
        "# Tokenize and preprocess documents\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "processed_docs = [\" \".join([word for word in tokenizer.tokenize(doc.lower()) if word not in stop_words]) for doc in df['Text (Clean)'].astype(str).tolist()]\n",
        "\n",
        "# Fit BERTopic model on the preprocessed text data\n",
        "topics, probabilities = topic_model.fit_transform(processed_docs)\n",
        "\n",
        "# Get top words per topic\n",
        "topics_info = topic_model.get_topics()\n",
        "top_words_per_topic = [[word for word, _ in words] for topic_id, words in topics_info.items() if topic_id != -1]\n",
        "\n",
        "# Tokenize each document separately\n",
        "tokenized_docs = [doc.split() for doc in processed_docs]\n",
        "\n",
        "# Create dictionary and corpus for coherence calculation\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Calculate coherence score\n",
        "coherence_model = CoherenceModel(topics=top_words_per_topic, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n",
        "\n",
        "# Determine the number of topics\n",
        "num_topics = len(topics_info) - 1 if -1 in topics_info else len(topics_info)\n",
        "print(f\"Number of Topics (K): {num_topics}\")\n",
        "\n",
        "# Summarize Topics\n",
        "print(\"\\nTopic Summaries:\")\n",
        "for topic_id, words in topics_info.items():\n",
        "    if topic_id != -1:\n",
        "        topic_summary = \", \".join([word for word, _ in words])\n",
        "        print(f\"Topic {topic_id}: {topic_summary}\\n\")\n",
        "    else:\n",
        "        break  # No more topics to print\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\"\"\"\n",
        "LDA : In the output it gives words with their probability whether it is positive or negative related to sentinment analysis\n",
        "positive means good or happy negative means bad or sad that sort of things\n",
        "LSA : In the output it takes sample text from a topic and relates whether it is positive or negative. The output is based on the coherence score\n",
        "lda2vec: In the output provided, each topic is summarized with key terms related to the topic, providing a concise overview of the themes.\n",
        "BERTopic: In the output provided, each topic is summarized with key terms, facilitating easy interpretation.\n",
        "LDA and LSA are faster among them LDA is easy.  LDA may be preferred when interpretability is crucial, while lda2vec and BERTopic may be suitable for capturing more complex relationships in the data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7d2949ec-9b02-4a7a-c482-75d620e637de"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLDA : In the output it gives words with their probability whether it is positive or negative related to sentinment analysis\\npositive means good or happy negative means bad or sad that sort of things\\nLSA : In the output it takes sample text from a topic and relates whether it is positive or negative. The output is based on the coherence score\\nlda2vec: In the output provided, each topic is summarized with key terms related to the topic, providing a concise overview of the themes.\\nBERTopic: In the output provided, each topic is summarized with key terms, facilitating easy interpretation.\\nLDA and LSA are faster\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Learning Experience:\n",
        "Overall, working with text data and implementing various topic modeling algorithms was a valuable learning experience. I gained a deeper understanding of how these algorithms work and their applications in extracting features from text data. The implementations provided clear examples of how to preprocess text, train topic models, and interpret the results. This hands-on approach helped me grasp the nuances of feature extraction from text data and understand the importance of parameter tuning and evaluation metrics in topic modeling.\n",
        "\n",
        "Challenges Encountered:\n",
        "One challenge I encountered was in understanding the nuances of each topic modeling algorithm and selecting the most appropriate one for the task at hand.\n",
        "Relevance to Your Field of Study:\n",
        "This exercise is highly relevant to the field of NLP, as topic modeling is a fundamental task in analyzing and understanding text data. By extracting features from text data, we can uncover hidden patterns, themes, and relationships that are crucial for various NLP applications such as document classification, information retrieval, and sentiment analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "c6dc9fad-d70c-4f32-98f0-333a7c21bd9c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLearning Experience:\\nOverall, working with text data and implementing various topic modeling algorithms was a valuable learning experience. I gained a deeper understanding of how these algorithms work and their applications in extracting features from text data. The implementations provided clear examples of how to preprocess text, train topic models, and interpret the results. This hands-on approach helped me grasp the nuances of feature extraction from text data and understand the importance of parameter tuning and evaluation metrics in topic modeling.\\n\\nChallenges Encountered:\\nOne challenge I encountered was in understanding the nuances of each topic modeling algorithm and selecting the most appropriate one for the task at hand. \\nRelevance to Your Field of Study:\\nThis exercise is highly relevant to the field of NLP, as topic modeling is a fundamental task in analyzing and understanding text data. By extracting features from text data, we can uncover hidden patterns, themes, and relationships that are crucial for various NLP applications such as document classification, information retrieval, and sentiment analysis. \\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "daab302550e64fea84067faa756b995c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce5dc7f7fcb249c583f9e260fded40e4",
              "IPY_MODEL_78404ea4a1f54ea78bfa9b65022dbd95",
              "IPY_MODEL_5ff90f66f7b6423f9894438fd9b92505"
            ],
            "layout": "IPY_MODEL_6d8f3e44fab3455885374eea56564925"
          }
        },
        "ce5dc7f7fcb249c583f9e260fded40e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96aef63cfc4a43f2bd88fa6565f3d614",
            "placeholder": "​",
            "style": "IPY_MODEL_3df2f7745467456d987beae97f2c24aa",
            "value": "Batches: 100%"
          }
        },
        "78404ea4a1f54ea78bfa9b65022dbd95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_743509651033474ebbe872b49597e95e",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e0d1db9953640d09b05ecf37240b944",
            "value": 31
          }
        },
        "5ff90f66f7b6423f9894438fd9b92505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5833f63d29c1425699d932b16fcc549a",
            "placeholder": "​",
            "style": "IPY_MODEL_255ff9c961804183b10289739b56857f",
            "value": " 31/31 [00:16&lt;00:00,  2.32it/s]"
          }
        },
        "6d8f3e44fab3455885374eea56564925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96aef63cfc4a43f2bd88fa6565f3d614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df2f7745467456d987beae97f2c24aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "743509651033474ebbe872b49597e95e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e0d1db9953640d09b05ecf37240b944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5833f63d29c1425699d932b16fcc549a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "255ff9c961804183b10289739b56857f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d773b678b3d8462c849ba2bb6763e694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5f7688f39474ed9a0c72584ba8d59f9",
              "IPY_MODEL_db698d5f26ef445c8cea1a1c259bba41",
              "IPY_MODEL_bc9719208b0741c38a8e88d8dcabd6a0"
            ],
            "layout": "IPY_MODEL_aa1457f4c5434d9389272a75b6fdee39"
          }
        },
        "b5f7688f39474ed9a0c72584ba8d59f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cdada3799434a4f83b1793a778c0c3e",
            "placeholder": "​",
            "style": "IPY_MODEL_b483e866c51b4a7f9b1f051c6e216ecf",
            "value": "Batches: 100%"
          }
        },
        "db698d5f26ef445c8cea1a1c259bba41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d7b72bb79f4e8bb9aa4f834b501975",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e25e0658dc740a2ab301ff08e997a14",
            "value": 31
          }
        },
        "bc9719208b0741c38a8e88d8dcabd6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8190cb0a00d44135a406c9c272a6789f",
            "placeholder": "​",
            "style": "IPY_MODEL_40dd08595ed940a88d197bc627b42bfa",
            "value": " 31/31 [00:21&lt;00:00,  2.78it/s]"
          }
        },
        "aa1457f4c5434d9389272a75b6fdee39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cdada3799434a4f83b1793a778c0c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b483e866c51b4a7f9b1f051c6e216ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24d7b72bb79f4e8bb9aa4f834b501975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e25e0658dc740a2ab301ff08e997a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8190cb0a00d44135a406c9c272a6789f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40dd08595ed940a88d197bc627b42bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}